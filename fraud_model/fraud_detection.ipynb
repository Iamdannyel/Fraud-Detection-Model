{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import logging\n",
        "import pyarrow\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import pickle\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "from sklearn.metrics import average_precision_score,precision_score, recall_score, f1_score, classification_report, confusion_matrix, roc_auc_score, precision_recall_curve\n",
        "from sklearn.utils import class_weight\n",
        "from src.model_functions import stratified_subsample, identify_repeat_perpetrator\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "AV8_1b6UFZjy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#get current working directory\n",
        "cwd = os.getcwd()\n",
        "image_dir = os.path.join(cwd, 'image_dir')\n",
        "\n",
        "if not os.path.exists(image_dir):\n",
        "    os.makedirs(image_dir)\n",
        "\n",
        "\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler('model_logfile.log'),  \n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#load file\n",
        "df = pd.read_parquet(r'C:\\Users\\chino\\Downloads\\fraud_model\\transactions_data\\PS_20174392719_1491204439457_log.parquet')"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "rxcyTuOSF55d",
        "outputId": "103c29a2-e164-4e24-f474-c479deef154a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df = df.drop('isFlaggedFraud', axis=1)"
      ],
      "outputs": [],
      "metadata": {
        "id": "KBhkI4oqGADu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.0 Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "qy3aTfXNwLZl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#check for null values\n",
        "df.isnull().sum()"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "YsJw32s1GCwV",
        "outputId": "81dfde6b-b789-42f4-9d1f-bd8a256b24d1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df.shape"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHbvVMnj04KB",
        "outputId": "ce9530e4-7e19-4a28-9ffc-6bca124eacd5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#summarise numerical columns\n",
        "df.describe()"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#chcck for correlation and identify potential feature multicolinearity\n",
        "df.corr()"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#investigate data types\n",
        "df.info()"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFvUZ77m2LcI",
        "outputId": "afca12f7-d3f7-4f0f-b86c-ca819bf1c35e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#convert isFraud to int to represent a binary categorical target variable\n",
        "df['isFraud'] = df['isFraud'].astype(int)"
      ],
      "outputs": [],
      "metadata": {
        "id": "2ogcV1EM2NyL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#check proportion of classes in the target variable\n",
        "class_count = df['isFraud'].value_counts()\n",
        "x = class_count.index\n",
        "y = class_count.values\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "plt.bar(x, y)\n",
        "\n",
        "# add value labels on top of each bar\n",
        "for i, v in enumerate(y):\n",
        "    plt.text(i, v + 500, str(v), ha='center', va='bottom')\n",
        "\n",
        "plt.xticks(x, ['Not Fraud', 'Is Fraud'])\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Class Distribution in Dataset')\n",
        "plt.show()\n",
        "class_count\n"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 725
        },
        "id": "0olPLEpDAuPa",
        "outputId": "43e6fe3a-7b00-43c2-af60-0eb25660af53"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df.duplicated().sum()"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pX4co7co9-6R",
        "outputId": "d844850a-5e7c-442a-faae-e08e10145360"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "numeric_features = ['amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']\n",
        "\n",
        "for col in numeric_features:\n",
        "    df[col] = df[col].astype(float)"
      ],
      "outputs": [],
      "metadata": {
        "id": "v1FQfBgvUc6b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "<br>"
      ],
      "metadata": {
        "id": "aR6Xx07jet8i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explore multi feature - target variable relationships to find useful trends"
      ],
      "metadata": {
        "id": "wpnQWqPxdpRb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#create subsample to enable seamless plotting\n",
        "df_subsample = stratified_subsample(df, frac = 0.1, target_col='isFraud')\n",
        "\n",
        "df_subsample['isFraud'] = df_subsample['isFraud'].astype(str)\n",
        "fig = px.scatter_matrix(df_subsample, dimensions=numeric_features, color='isFraud', height=1000, color_discrete_map={'1': 'red', '0' : 'green'})\n",
        "fig.update_traces(diagonal_visible=False)\n",
        "fig.show()\n"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EnIafy-HXb88",
        "outputId": "be089690-65b3-4191-a92a-9f484efd5ae2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fraudulent Transactions by Transaction Type"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "fraud_counts = df_subsample.groupby('type')['isFraud'].value_counts()\n",
        "fraud_counts_df = fraud_counts.reset_index(name='count')\n",
        "palette = {'0': 'green', '1':'red', 0: 'green', 1:'red'}\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "ax = sns.barplot(x='type', y='count', hue='isFraud', data=fraud_counts_df, palette=palette)\n",
        "# Add labels on top of the bars\n",
        "for p in ax.patches:\n",
        "    height = p.get_height()\n",
        "    if height > 0:\n",
        "        ax.annotate(f'{int(height)}', \n",
        "                    (p.get_x() + p.get_width() / 2, height), \n",
        "                    ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "plt.title('Fraudulent Transactions by Transaction Type')\n",
        "plt.xlabel('Transaction Type')\n",
        "plt.ylabel('Number of Transactions')\n",
        "#lt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "ktAE3kUIsDNQ",
        "outputId": "6e1ea5a7-5e8a-4ccd-a9af-c666e871bcfa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transaction Amount By Transaction Type (Fraud and Non-Fraud)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "for tx_type in ['CASH_OUT', 'TRANSFER']:\n",
        "    df_type = df_subsample[df_subsample['type'] == tx_type]\n",
        "    \n",
        "    fig, axes = plt.subplots(nrows=len(numeric_features), ncols=1, figsize=(10, 4 * len(numeric_features)))\n",
        "    axes = np.array(axes).flatten() if len(numeric_features) > 1 else [axes]\n",
        "    \n",
        "    for i, feature in enumerate(numeric_features):\n",
        "        df_type[f'log_{feature}'] = np.log10(df_type[feature] + 1)\n",
        "        \n",
        "        # Create violin plot\n",
        "        sns.violinplot(\n",
        "            x='isFraud',\n",
        "            y=f'log_{feature}',\n",
        "            data=df_type,\n",
        "            ax=axes[i],\n",
        "            split=False,\n",
        "            palette={'0': 'green', '1': 'red'}  # Green for non-fraud, red for fraud\n",
        "        )\n",
        "        axes[i].set_title(f'{feature} Distribution for {tx_type} by Fraud Status')\n",
        "        axes[i].set_xlabel('Fraud Status')\n",
        "        axes[i].set_ylabel(f'Log {feature} (log10)')\n",
        "        #axes[i].set_xticks([1,0], ['Fraud', 'Non Fraud'])  # Change x-axis labels\n",
        "        \n",
        "    plt.tight_layout()\n",
        "    os.makedirs(image_dir, exist_ok=True)\n",
        "    plt.savefig(f'{image_dir}/violin_plots_{tx_type.lower()}.png')\n",
        "    plt.show()\n",
        "    plt.close()  # Close to free memory\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mean/median of numeric features by fraud status"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "\n",
        "\n",
        "stats = []\n",
        "for col in numeric_features:\n",
        "    group_stats = df.groupby('isFraud')[col].agg(['mean', 'median']).reset_index()\n",
        "    group_stats['Column'] = col\n",
        "    group_stats = group_stats.melt(id_vars=['isFraud', 'Column'], \n",
        "                                  value_vars=['mean', 'median'], \n",
        "                                  var_name='Statistic', \n",
        "                                  value_name='Value')\n",
        "    stats.append(group_stats)\n",
        "\n",
        "stats_df = pd.concat(stats)\n",
        "\n",
        "num_cols = len(numeric_features)\n",
        "nrows = (num_cols + 1) // 2\n",
        "fig, axes = plt.subplots(nrows=nrows, ncols=2, figsize=(16, 4 * nrows))\n",
        "axes = axes.flatten()\n",
        "    \n",
        "for i, col in enumerate(numeric_features):\n",
        "        col_data = stats_df[stats_df['Column'] == col]\n",
        "        ax = axes[i]\n",
        "        sns.barplot(x='isFraud', y='Value', hue='Statistic', data=col_data, ax=ax)\n",
        "        ax.set_title(f'Mean and Median of {col} by Fraud Status')\n",
        "        #ax.set_xlabel('Fraudulent Transaction (0 = Non-Fraud, 1 = Fraud)')\n",
        "        ax.set_ylabel('Amount ($)')\n",
        "        #ax.legend(title='Statistic')\n",
        "        \n",
        "        # add value labels in millions\n",
        "        for p in ax.patches:\n",
        "            height = p.get_height()\n",
        "            if np.isfinite(height):\n",
        "                value_millions = height / 1_000_000\n",
        "                ax.text(\n",
        "                    p.get_x() + p.get_width() / 2,\n",
        "                    height,\n",
        "                    f'${value_millions:.2f}M',\n",
        "                    ha='center',\n",
        "                    va='bottom',\n",
        "                    fontsize=10\n",
        "                )\n",
        "    \n",
        "for j in range(i + 1, len(axes)):\n",
        "        fig.delaxes(axes[j])\n",
        "    \n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "plt.savefig(f'{image_dir}/stats_barplot.png')\n",
        "#plt.close()\n",
        "#logging.info(f\"Barplot saved to {image_dir}/stats_barplot.png\")"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Time-based fraud analysis"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#df = df_subsample.copy()\n",
        "\n",
        "df['hour'] = df['step'] % 24\n",
        "\n",
        "hourly_counts = df.groupby(['hour', 'isFraud']).size().reset_index(name='count')\n",
        "\n",
        "# compute total transactions per hour\n",
        "total_per_hour = hourly_counts.groupby('hour')['count'].transform('sum')\n",
        "\n",
        "# add proportion column\n",
        "hourly_counts['proportion'] = hourly_counts['count'] / total_per_hour\n",
        "\n",
        "# plot proportion lineplot\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(data=hourly_counts, x='hour', y='proportion', hue='isFraud', marker='o', palette= palette)\n",
        "plt.title('Proportion of Fraudulent vs Non-Fraudulent Transactions by Hour')\n",
        "plt.xlabel('Hour of Day')\n",
        "plt.ylabel('Proportion of Transactions')\n",
        "plt.xticks(range(0, 24))\n",
        "plt.legend(title='Is Fraud', labels=['Non-Fraud', 'Fraud'])\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "# Save plot\n",
        "plt.savefig(f'{image_dir}/hour_vs_fraud_proportion.png')\n",
        "plt.close()\n",
        "logging.info(\"Proportion plot by hour saved.\")\n"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "Tx7NJJHqufbq",
        "outputId": "786ad46b-23c0-4a1e-b580-0f44c3c68da7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df['tx_type'] = df.apply(lambda x: x['nameOrig'][0] + '-' + x['nameDest'][0], axis=1)\n",
        "df.groupby('tx_type')['isFraud'].value_counts()"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#check for repeat perpetrators  and victims \n",
        "\n",
        "fraud_df = df[df['isFraud'] == 1]\n",
        "\n",
        "# count repeat perpetrators\n",
        "repeat_perpetrators = fraud_df['nameDest'].value_counts()\n",
        "num_repeat_perpetrators = (repeat_perpetrators > 1).sum()\n",
        "\n",
        "# count repeat victims\n",
        "repeat_victims = fraud_df['nameOrig'].value_counts()\n",
        "num_repeat_victims = (repeat_victims > 1).sum()\n",
        "\n",
        "print(f\"Number of repeat perpetrators: {num_repeat_perpetrators}\")\n",
        "print(f\"Most number of fraudulent transactions from a single perpetrator: {repeat_perpetrators.values[0]}\")\n",
        "print(f\"Number of repeat victims: {num_repeat_victims}\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "BzPxWqdQyTEz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Key observations from Exploratory Data Analysis**\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- Severe class imbalance due to rarity of fradulent transactions\n",
        "- Fraud only occurs in 'Cash Out' and 'Transfer' transactions, suggesting that transaction type is a critical predictor.\n",
        "- Fraudulent transactions often involve large amounts and drained accounts as observed in the disparity in mean/median amount and balances between fraudulent and non-fraudulent transactions\n",
        "- Fraudulent transactions are more prevalent from 2AM, peak around 4-5AM and then experience a decline.\n",
        "- There are no repeat victims and 44 repeat perpetrators  \n",
        "- No fraudulent transaction involves merchant \n",
        "- In fraudulent transactions, the transaction amount often equates to the origin's balance when the balance is less than $10m, in which case the balance becomes $0. When the origin's balance is greater than $10m, $10m is transferred out of the account, leaving the surplus. This trend suggests that the maximum transaction limit without any higher-level authentication is $10m. As a result, transactions that leave the origin balance as zero and those maxxing out the limit are likely to be fraudulent."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "<br>"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.0 Feature Engineering"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### There are 44 repeat perpetrators as observed in EDA. This is useful information, but must be used cautiously to avoid data leakage because it mirrors the target variable. For instance, if it is added directly to the dataset as a feature, and a nameDest commits fraud at step = 100, flagging it as a repeat perpetrator in a row at step = 50 would use future knowledge (isFraud = 1 at step = 100), which isn’t available in real-time fraud detection. This would cause generalisation issues to the model.\n",
        "\n",
        "#### The ideal way to create a feature to flag repeat perpetrators without leakage is to use the step column to ensure only past fraud (before the current row’s step) is considered. This involves create a rolling history of fraudulent nameDest accounts up to each step. This is implemented in the 'identify repeat perpetrator' function. \n",
        "\n",
        "#### Similar ideology is applied to all newly created features, ensuring that the development and model application mimicks real world setting where future fraud is unknown. Also, some newly created features were taken out due to the fact that they set very linear 'fraud recognition' patterns for the model to follow, which makes generalisation impossible and leads to inflated evaluation results.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#commented out due to high multicolinearity with target variable \n",
        "#df['is_balance_drained'] = df.apply(lambda x: 1 if x['oldbalanceOrg'] == x['amount'] else 0, axis=1)\n",
        "#df['max_tx_amount'] =  df['amount'].apply(lambda x: 1 if x == 10000000.00 else 0)\n",
        "\n",
        "# sort by step to ensure global temporal order\n",
        "df = df.sort_values('step').reset_index(drop=True)\n",
        "\n",
        "# log transformation\n",
        "df['log_amount'] = np.log1p(df['amount'])\n",
        "\n",
        "# 1. rolling transaction count (last 6 time steps per sender)\n",
        "df['tx_count_6hr'] = (\n",
        "    df.groupby('nameOrig')['step']\n",
        "      .rolling(window=6, min_periods=1)\n",
        "      .count()\n",
        "      .reset_index(level=0, drop=True)\n",
        ")\n",
        "\n",
        "\n",
        "# cumulative amount sent by sender\n",
        "df['cum_amount_sent'] = df.groupby('nameOrig')['log_amount'].cumsum()\n",
        "\n",
        "#ender mean, std, and Z-score (optimized)\n",
        "df = df.sort_values(['nameOrig', 'step'])  # Sort for sender-specific features\n",
        "df['sender_mean'] = (\n",
        "    df.groupby('nameOrig')['log_amount']\n",
        "      .expanding()\n",
        "      .mean()\n",
        "      .shift(1)\n",
        "      .reset_index(level=0, drop=True)\n",
        ")\n",
        "df['sender_std'] = (\n",
        "    df.groupby('nameOrig')['log_amount']\n",
        "      .expanding()\n",
        "      .std()\n",
        "      .shift(1)\n",
        "      .reset_index(level=0, drop=True)\n",
        ")\n",
        "df['sender_std'] = df['sender_std'].fillna(1).replace(0, 1)\n",
        "\n",
        "#identify outlier transaction based on user's history\n",
        "df['zscore_amount'] = (df['log_amount'] - df['sender_mean']) / df['sender_std']\n",
        "\n",
        "#  time since last transaction\n",
        "df['prev_step'] = df.groupby('nameOrig')['step'].shift(1)\n",
        "df['time_since_last_tx'] = df['step'] - df['prev_step']\n",
        "df['is_first_tx'] = df['prev_step'].isna().astype(int)\n",
        "df['time_since_last_tx'] = df['time_since_last_tx'].fillna(df['time_since_last_tx'].median())\n",
        "\n",
        "df = df.drop(['prev_step'], axis=1)\n",
        "df = df.dropna()\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "numeric_features = [\n",
        "    'hour', 'log_amount', 'tx_count_6hr',\n",
        "    'cum_amount_sent', 'oldbalanceOrg', 'zscore_amount', \n",
        "    'time_since_last_tx'\n",
        "]\n",
        "\n",
        "categorical_features = ['type', 'tx_type']  # one-hot encode separately\n",
        "\n",
        "\n",
        "\n",
        "x = df.drop(['isFraud'], axis=1)\n",
        "y = df['isFraud']\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test  = train_test_split(x, y, test_size = 0.3, random_state = 42, stratify=y)\n",
        "\n",
        "fraud_ratio = y_train.mean()\n",
        "scale_pos_weight = (1 - fraud_ratio) / fraud_ratio \n",
        "logging.info(f\"Scale_pos_weight: {scale_pos_weight:.2f}\")\n",
        "\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numeric_features),\n",
        "        ('cat', OneHotEncoder(sparse=False, handle_unknown='ignore', drop='first'), categorical_features),\n",
        "       # ('pca', PCA(n_components = 5))\n",
        "\n",
        "        #('binary', 'passthrough', binary_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "X_train = preprocessor.fit_transform(X_train)\n",
        "X_test = preprocessor.transform(X_test)\n",
        "\n",
        "\n",
        "model = XGBClassifier(\n",
        "    n_estimators=100,\n",
        "    scale_pos_weight=scale_pos_weight,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=10,\n",
        "    use_label_encoder=False,  \n",
        "    eval_metric='logloss')    \n",
        "\n",
        "\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_prob = model.predict_proba(X_test)[:, 1]  # probabilities for class 1\n",
        "\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_prob)\n",
        "\n",
        "#plot precision and recall vs. threshold\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(thresholds, precision[:-1], label='Precision', color='b')\n",
        "plt.plot(thresholds, recall[:-1], label='Recall', color='r')\n",
        "plt.xlabel('Threshold')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Precision and Recall vs Threshold')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.0 Model Evaluation"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "f1_scores = 2 * (precision * recall) / (precision + recall + 1e-6)\n",
        "best_index = np.argmax(f1_scores)\n",
        "best_threshold = thresholds[best_index]\n",
        "\n",
        "print(f\"Best Threshold: {best_threshold:.4f}\")\n",
        "print(f\"Precision: {precision[best_index]:.4f}, Recall: {recall[best_index]:.4f}, F1: {f1_scores[best_index]:.4f}\")\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# convert probabilities to binary using the best threshold\n",
        "y_pred_thresh = (y_prob >= best_threshold).astype(int)\n",
        "\n",
        "# confusion Matrix and Classification Report\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_thresh))\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_thresh))\n",
        "\n",
        "\n",
        "print(\"AUC-ROC:\", roc_auc_score(y_test, y_prob))\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test, y_pred_thresh)\n",
        "\n",
        "ax = sns.heatmap(conf_matrix, cmap='flare', annot=True, fmt='d')\n",
        "\n",
        "plt.xlabel('Predicted Class', fontsize=11)\n",
        "plt.ylabel('True Class', fontsize = 11)\n",
        "plt.title('Confusion Matrix', fontsize=11)\n",
        "plt.show()\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Result Summary \n",
        "\n",
        "Precision (0.81 for class 1): Among all transactions flagged as fraudulent by the model, 81% were actually fraudulent. This suggests the model has a decent ability to identify true frauds.\n",
        "\n",
        "Recall (0.76 for class 1): The model identified 76% of all actual fraudulent transactions and missed only 24% of fraudulent transactions.\n",
        "\n",
        "F1-Score (0.78 for class 1): The F1-score is a harmonic mean of precision and recall. At 0.78, this indicates that the model strikes a decent balance between detecting fraud and avoiding false positives."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "<br>"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### We can apply RandomisedSearchCV to search for the potentially better hyperparameter values to retrain the model for improved performance. GridSearch would be more ideal if computational resources and memory were not constraints"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "\n",
        "\n",
        "X_tune, _, y_tune, _ = train_test_split(X_train, y_train, train_size=0.1, random_state=42, stratify=y_train)\n",
        "print(\"Tuning subset shape:\", X_tune.shape, y_tune.shape)\n",
        "\n",
        "numeric_features = X_tune.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "categorical_features = X_tune.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "print(\"Numeric Features:\", numeric_features)\n",
        "print(\"Categorical Features:\", categorical_features)\n",
        "\n",
        "\n",
        "# compute scale_pos_weight for sampled data\n",
        "neg, pos = np.bincount(y_tune)\n",
        "scale_pos_weight = neg / pos\n",
        "print(\"scale_pos_weight:\", scale_pos_weight)\n",
        "\n",
        "xgb_model = XGBClassifier(scale_pos_weight=scale_pos_weight, random_state=42, eval_metric='logloss')\n",
        "param_grid = {\n",
        "    'xgbclassifier__learning_rate': [0.01, 0.1, 0.3],\n",
        "    'xgbclassifier__max_depth': [3, 5, 7],\n",
        "    'xgbclassifier__n_estimators': [50, 100],\n",
        "    'xgbclassifier__subsample': [0.8, 1.0],\n",
        "    'xgbclassifier__colsample_bytree': [0.8, 1.0]\n",
        "}\n",
        "\n",
        "#create a pipeline and perform randomised search CV\n",
        "pipeline = make_pipeline(preprocessor, xgb_model)\n",
        "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "grid_search = RandomizedSearchCV(pipeline, param_grid, cv=kf, scoring='recall', n_jobs=-1, verbose=1)\n",
        "grid_search.fit(X_tune, y_tune)\n",
        "\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best Recall Score:\", grid_search.best_score_)\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results from best parameters are not as good as our initial results so no need to retrain the model on the output hyperparameter values\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "\n",
        "\n",
        "# specify the file path where you want to save the model\n",
        "file_path = os.path.join(cwd,'fraud_xgbmodel.pkl') \n",
        "\n",
        "# Save the model to a pickle file\n",
        "with open(file_path, 'wb') as file:\n",
        "    pickle.dump(model, file)\n",
        "\n",
        "print(f\"Model saved successfully to {file_path}\")"
      ],
      "outputs": [],
      "metadata": {}
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}